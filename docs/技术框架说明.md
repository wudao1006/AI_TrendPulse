# AI Public Opinion 系统技术架构白皮书

**版本**: 1.0.0  
**最后更新**: 2026-01-16  
**维护者**: AI舆论项目组

---

## 1. 系统概述 (System Overview)

### 1.1 项目背景
在信息爆炸时代，社交媒体上的舆论瞬息万变。传统的舆情系统通常基于关键词匹配（Keyword Matching），存在以下痛点：
*   **语义理解差**: 无法区分反讽、双关语。
*   **信息噪音大**: 无法有效过滤重复、低价值的“灌水”内容。
*   **报告生成慢**: 依赖人工阅读和总结。

**AI Public Opinion** 旨在通过结合 **本地小模型 (Embedding)** 的高通量处理能力与 **云端大模型 (LLM)** 的深度理解能力，构建一个自动化、实时化、可视化的舆情分析闭环。

### 1.2 核心能力
1.  **全网采集**: 覆盖 X (Twitter)、YouTube、Reddit 三大主流舆论场。
2.  **抗反爬**: 独创的基于 Playwright 的账号池轮询机制，模拟人类行为。
3.  **智能分析**: 情感倾向分析、观点聚类、热度指数计算。
4.  **思维可视化**: 自动生成 Mermaid 思维导图，还原舆论逻辑链路。

---

## 2. 系统架构设计 (Architecture Design)

系统采用 **Micro-kernel (微内核)** 思想构建的单体应用，前后端分离，计算与 I/O 分离。

### 2.1 技术栈选型 (Tech Stack)

| 模块 | 技术选型 | 选型理由 |
| :--- | :--- | :--- |
| **Backend** | **FastAPI** | 原生支持异步 (AsyncIO)，性能是 Flask 的 3-5 倍，自动生成 OpenAPI 文档。 |
| **Task Queue** | **Celery + Redis** | 工业级分布式任务队列，支持任务编排 (Chord/Chain) 和重试机制。 |
| **Database** | **PostgreSQL** | 强大的 JSONB 支持（存储原始采集数据），事务性强。 |
| **ORM** | **SQLAlchemy 2.0** | 现代化 Python ORM，支持异步查询。 |
| **Frontend** | **Flutter** | 一套代码编译 Web、Windows、Android，开发效率高，Riverpod 状态管理。 |
| **AI (Local)** | **Sentence-Transformers** | 本地运行 `intfloat/multilingual-e5-small`，零成本处理海量文本去重。 |
| **AI (Cloud)** | **OpenAI Interface** | 兼容所有 OpenAI 格式接口（DeepSeek, GPT-4, Claude），灵活切换模型。 |

### 2.2 逻辑架构图

```mermaid
graph TB
    subgraph "Frontend Layer (Flutter)"
        UI[User Interface]
        State[Riverpod State Manager]
        Net[Dio HTTP Client]
    end

    subgraph "API Gateway (FastAPI)"
        Auth[API Key Auth]
        Routes[Task/Platform/Sub Routers]
        Schema[Pydantic Validators]
    end

    subgraph "Service Layer"
        Scheduler[APScheduler Service]
        DB_Service[CRUD Services]
    end

    subgraph "Worker Cluster (Celery)"
        Orchestrator[Task Orchestrator (Chord)]
        
        subgraph "Collectors"
            X_Bot[X/Twitter Scraper (Playwright)]
            YT_Bot[YouTube API + Transcript]
            RD_Bot[Reddit API + HTTP]
        end
        
        subgraph "Analyzers"
            Pre[Data Preprocessor]
            Embed[Embedding Sampler (Local AI)]
            LLM[LLM Agent (Cloud AI)]
            Mermaid[Mindmap Generator]
        end
    end

    subgraph "Data Persistence"
        Redis[(Redis Broker/Cache)]
        PG[(PostgreSQL)]
    end

    UI -->|REST API| Auth
    Auth --> Routes
    Routes --> Orchestrator
    Orchestrator --> Redis
    Redis --> Collectors
    Redis --> Analyzers
    Collectors --> PG
    Analyzers --> PG
    Analyzers --> LLM
    Routes --> PG
```

---

## 3. 核心模块详解 (Core Modules)

### 3.1 采集器工厂 (Collector Registry)
所有采集器继承自 `BaseCollector` 抽象基类，确保统一的接口规范。

#### A. X (Twitter) 采集器 (`app/collectors/x.py`)
这是系统中最复杂的采集器，解决了 X 平台严格的反爬策略。
*   **架构**: 基于 `Playwright` 的无头浏览器 (Chromium)。
*   **账号池 (`XAccountPool`)**:
    *   支持加载 `JSON` 格式的多个账号 Cookie。
    *   **自动轮换**: 每次采集任务自动切换账号。
    *   **熔断机制**: 单个账号连续失败 3 次自动标记为 `paused`。
*   **采集深度**:
    *   **第一层**: 搜索结果页 (Search Timeline)。
    *   **第二层**: 自动点击进入推文详情页，抓取一级评论。
    *   **第三层**: (可选) 递归抓取二级回复 (`reply_depth` 参数)。
*   **反爬对抗**:
    *   **Jitter**: 随机的滚动停顿时间 (0.7s - 1.5s)。
    *   **Stagnation Detection**: 自动检测页面是否停止加载（判定为到底或被封控）。

#### B. YouTube 采集器 (`app/collectors/youtube.py`)
*   **策略**: "元数据 + 字幕" 双轨制。
*   **流程**:
    1.  使用官方 Data API 搜索视频，获取标题、播放量、发布时间。
    2.  使用 `youtube-transcript-api` 获取视频外挂/自动生成字幕。
    3.  **智能切片**: 将长字幕按 **300秒** 为单位切分为多个 `CollectedItem`，确保 LLM 的 Context Window 不会溢出。

### 3.2 AI 分析管道 (`app/workers/analyze_tasks.py`)

分析过程是一个典型的 **Map-Reduce** 过程，分为三个阶段：

#### 阶段一：流式预处理与智能采样 (Streaming & Sampling)
*   **挑战**: 热门话题可能包含 10w+ 条数据，全部送入 LLM 既昂贵又会超时。
*   **解决方案**:
    1.  **流式读取**: 使用 `db.yield_per(500)` 避免一次性加载所有数据导致 OOM。
    2.  **清洗**: 正则去除 URL、表情符、空白字符。
    3.  **热度加权**: 根据点赞/评论数计算每条数据的基础权重。
    4.  **智能采样 (Semantic Sampling)**:
        *   使用本地模型 `multilingual-e5-small` 将文本转为 384维向量。
        *   计算向量相似度，**去除语义重复**的数据（如大量转发的相同文案）。
        *   使用 **最大边际相关性 (MMR)** 算法或基于密度的聚类，选出最具代表性的 Top-N 条观点。

#### 阶段二：LLM 深度分析 (LLM Processing)
系统与 LLM 进行两次主要交互：
1.  **情感打分 (Sentiment Analysis)**:
    *   **Batching**: 每 10-20 条评论打包为一个 Prompt，减少 HTTP 请求开销。
    *   **Output**: 要求 JSON 格式，包含每条评论的 `score` (0-100) 和 `key_phrases`。
2.  **观点聚类 (Clustering)**:
    *   **Input**: 采样后的高价值评论 + 情感分布数据。
    *   **Prompt**: 要求 LLM 生成 2-6 个核心观点，每个观点包含 "Title", "Description", "Points" (论据)。
    *   **Self-Correction**: 独创的 **JSON 修复回路**。如果 LLM 返回的 JSON 无法解析，系统会自动将错误信息反馈给 LLM 要求重写，重试最多 2 次。

#### 阶段三：可视化生成 (Visualization)
*   **Mermaid Generator**: 将结构化的观点数据转化为 Mermaid Mindmap 语法。
*   **热度指数算法**:
    $$ Heat = 0.6 \times S_{engagement} + 0.25 \times S_{volume} + 0.15 \times S_{platform} $$
    *   其中 $S_{engagement}$ 引入了 **时间衰减 (Time Decay)** 因子，旧新闻的热度权重会随时间指数级下降（半衰期 24h）。

---

## 4. 数据模型设计 (Database Schema)

### 4.1 Tasks 表
核心任务表，记录任务状态。
| 字段 | 类型 | 说明 |
| :--- | :--- | :--- |
| `id` | UUID | 主键 |
| `status` | Enum | pending, running, completed, failed |
| `platforms` | JSON | 包含的平台列表 ["x", "reddit"] |
| `platform_configs`| JSON | 各平台的特定配置（如采集深度） |
| `celery_task_id` | String | 关联的异步任务 ID |

### 4.2 RawData 表
存储未经处理的原始采集数据，通常数据量巨大。
| 字段 | 类型 | 说明 |
| :--- | :--- | :--- |
| `task_id` | UUID | 外键 |
| `source_id` | String | 平台原始 ID (如 Tweet ID) |
| `content` | Text | 原始内容 |
| `metrics` | JSONB | 点赞、转发、评论数 (用于热度计算) |
| `extra_fields` | JSONB | 存储平台特有字段 (如 YouTube 频道 ID) |

### 4.3 AnalysisResult 表
存储 AI 分析后的精炼结果。
| 字段 | 类型 | 说明 |
| :--- | :--- | :--- |
| `sentiment_score` | Int | 加权情感总分 (0-100) |
| `key_opinions` | JSONB | 结构化的观点列表 |
| `mermaid_code` | Text | 思维导图源码 |
| `heat_index` | Float | 热度指数 |

---

## 5. 配置与运维 (Configuration & Ops)

### 5.1 环境变量说明 (`.env`)

**基础配置**
*   `DATABASE_URL`: PostgreSQL 连接串 `postgresql://user:pass@host:port/db`
*   `REDIS_URL`: Redis 连接串 `redis://:pass@host:port/0`
*   `API_KEY`: 后端 API 的鉴权密钥 (Client 请求需带 `Authorization: Bearer <API_KEY>`)

**第三方服务**
*   `LLM_API_KEY`: OpenAI/DeepSeek API Key
*   `LLM_MODEL`: 模型名称 (推荐 `gpt-4-turbo` 或 `deepseek-chat`)
*   `REDDIT_CLIENT_ID` / `REDDIT_CLIENT_SECRET`: Reddit API 凭证
*   `YOUTUBE_API_KEY`: Google Cloud API Key

**X (Twitter) 专用**
*   `X_ACCOUNTS_JSON`: JSON 字符串格式的账号列表（包含 Cookie）。
*   `X_HEADLESS`: `true` (无头模式) 或 `false` (调试模式)。

### 5.2 常见问题排查 (Troubleshooting)

**Q: Celery Worker 启动报错 "OOM / Killed"?**
*   **A**: 检查是否开启了流式处理。确认 `analyze_tasks.py` 中使用了 `yield_per`。对于极小内存机器 (512MB)，请减小 `semantic_sampling_batch_size` 至 16。

**Q: X 采集器一直返回空数据?**
*   **A**: 
    1.  检查 `X_ACCOUNTS_JSON` 中的 Cookie 是否过期。
    2.  设置 `X_HEADLESS=false` 本地运行，观察浏览器是否弹出验证码。
    3.  确认网络环境是否能访问 x.com。

**Q: LLM 分析结果全是 "JSON Parse Error"?**
*   **A**: 
    1.  使用的模型可能太弱（如 7B 以下小模型）。
    2.  检查 `LLM_API_BASE_URL` 是否配置正确。
    3.  系统会自动尝试修复，如果修复也失败，建议更换更强的模型。

---
**End of Document**
